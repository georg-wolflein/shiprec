cluster:
  n_cpu_workers: 8 # number of CPU workers
  gpu_devices: [] # list of GPU devices to use (will spawn one extra worker per GPU)
                  # if empty, feature extraction will be done on CPU
                  # if not empty, feature extraction will be done *only* with the GPU workers
  memory_limit: 32GB # memory limit per worker
input:
  path: # path to folder containing slides
  glob: "*.svs" # glob pattern for slides
  slide_backend: cucim # supported options: openslide, cucim
pipeline:
  n_parallel_slides: 1 # number of slides to process in parallel
  patch_size: 224
  target_mpp: ${eval:256.0 / 224.0}
  stain_normalization:
    enabled: true
    method: macenko # supported options: macenko
    batch_size: 256 # number of patches per chunk for Macenko normalization
    template: normalization_template.jpg # path to template for Macenko normalization
  feature_extraction:
    batch_size: ${pipeline.stain_normalization.batch_size}  # recommended to be a multiple or divisor of feature extraction batch size
output:
  path: # path to output folder
  format: zarr # supported options: h5py, zarr
  features:
    save: true # whether to save the extracted features
  coords:
    save: true # whether to save the coordinates of the patches
  patch_grid:
    save: true # whether to save a grid of patch indices
  patches:
    save: false # whether to save the patches
  normalized_patches:
    save: true # whether to save the normalized patches
